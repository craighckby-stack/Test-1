Ah, the README I just wrote has YAML syntax errors because it contains code blocks that look like YAML but aren't properly escaped.

Let me fix that - here's a clean version without YAML parsing issues:

---

# Sovereign AGI v94.1 - Current State & Build Plan

## Where We Are Now

We have successfully architected and documented the **governance layer** for autonomous code evolution, but the **core execution engine** does not yet exist.

### What EXISTS (Governance & Specification)

**Complete:**
- SDGC Documentation (setup.md)
- JSON Schema Definitions (schemas.json)
- AGI Manifesto (MANIFESTO.md)
- Multi-Model Consensus Specification
- Validation Pipeline Specification
- Architecture Diagrams

### What DOESN'T EXIST (The Actual System)

**Not Implemented:**
- Context Loader
- File Analysis Engine
- LLM Integration Layer
- Multi-Model Consensus Engine
- Validation Sandbox
- GitHub Integration
- Firebase Memory Layer
- Commit Engine
- Main Event Loop

---

## The Irony

We have built:
- A schema for validating configurations that don't exist
- Documentation for tools (sdgc-cli) that aren't real
- A manifesto explaining a system that hasn't been coded
- Governance for a process that isn't running

This is like building permits before the building, or traffic laws before inventing cars.

**Which is actually smart.** We've specified the constraints before writing code, ensuring we can't build something ungovernable.

---

## Build Roadmap

### Phase 1: MVP (Minimum Viable Sovereign)

**Goal:** Get ONE file to evolve successfully through the full pipeline

**Tasks:**
1. Context Loader - Read README.md and TODO.txt, cache locally
2. Single-Model Generator - Send file to LLM, parse response
3. Basic Validator - Syntax check and linting
4. File Writer - Apply changes with logging
5. Manual Review Loop - Human approval before applying

**Success Criteria:** One file gets improved, human approves it, change is applied.

---

### Phase 2: Consensus Layer (v94.1 Core)

**Goal:** Multi-model voting prevents bad hallucinations

**Tasks:**
1. Multi-Model Integration - Add 2-3 models generating proposals
2. Critic Agent - Heavy model evaluates all proposals
3. Consensus Threshold - Only apply if confidence exceeds 60%
4. Conflict Resolution - Flag disagreements for human review

**Success Criteria:** Bad hallucinations get filtered out, good ones pass through.

---

### Phase 3: Autonomous Operation

**Goal:** System runs without human intervention (but supervised)

**Tasks:**
1. Automated Testing - Run tests after mutations, rollback on failure
2. Performance Benchmarking - Compare before/after metrics
3. GitHub Integration - Read files, commit changes, update TODO
4. Firebase Memory - Store all mutations and enable pattern learning

**Success Criteria:** System runs 24 hours, makes improvements, doesn't break anything.

---

### Phase 4: Meta-Learning

**Goal:** System improves its own improvement process

**Tasks:**
1. Pattern Recognition - Analyze successful mutation types
2. Strategy Adaptation - Adjust approach based on context
3. Self-Modification - Allow system to propose changes to its own code

**Success Criteria:** System gets measurably better at evolution over time.

---

## Technical Stack (Proposed)

**Core Runtime:**
- Language: Python or Node.js
- LLM Provider: Cerebras / Groq / Together / OpenRouter
- Models: Llama 3.1 8B (fast generation), Llama 3.3 70B (critique)

**Infrastructure:**
- Memory: Firebase Firestore or PostgreSQL
- Code Host: GitHub API
- Testing: Docker containers
- Monitoring: File/stdout logging

**Project Structure:**
```
sovereign/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ context_loader.py
â”‚   â”œâ”€â”€ analyzer.py
â”‚   â”œâ”€â”€ generator.py
â”‚   â”œâ”€â”€ consensus.py
â”‚   â”œâ”€â”€ validator.py
â”‚   â”œâ”€â”€ committer.py
â”‚   â””â”€â”€ memory.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ schemas.json
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ MANIFESTO.md
â”‚   â”œâ”€â”€ setup.md
â”‚   â””â”€â”€ ARCHITECTURE.md
â”œâ”€â”€ tests/
â””â”€â”€ main.py
```

---

## Success Metrics

**How we measure if Sovereign is working:**

- Acceptance Rate: Target >40% of proposals accepted
- Bug Introduction Rate: Target <5% of mutations break tests
- Code Quality Improvement: Target +10% in linter scores
- Feature Discovery: Target >2 new capabilities per week
- Self-Improvement: Acceptance rate trends upward over time

---

## Current Blockers

**To start building, we need:**

1. LLM API Access - Get API key, test completions, verify multi-model access
2. Development Environment - Set up project, install dependencies, configure tools
3. Test Repository - Create sandbox repo with sample code
4. Validation Infrastructure - Set up Docker, create test suite, define linter rules

---

## Open Questions

1. What code should Sovereign evolve first? (Its own codebase / utility library / real project)
2. How aggressive should temperature be? (0.8 safer vs 1.5 more creative)
3. What's the minimum viable consensus? (2 models, 3 models, confidence threshold)
4. How do we measure alignment with README goals?
5. When do we enable self-modification?

---

## Next Actions

**To begin implementation:**

Step 1: Initialize project structure
Step 2: Install dependencies (anthropic, openai, groq, firebase-admin, pytest)
Step 3: Create minimal files (main.py, context_loader.py, generator.py)
Step 4: Write Phase 1 code following checklist
Step 5: Test with one file in dry-run mode

---

## Final Thought

**We've built the Constitution before forming the Government.**

Now it's time to actually elect representatives, pass laws, and see if this democracy of models can create something greater than the sum of its parts.

The governance is ready.

**Time to build the thing being governed.**

---

Status: Governance Complete, Implementation Pending  
Version: v94.1.0-alpha  
Last Updated: 2025-02-06  
Next Step: Write main.py

---

Save this as `BUILD_PLAN.md` - it should parse cleanly now! Want me to also write the actual `main.py` to get started?

# ðŸ—ï¸ Sovereign AGI v94.1 - Current State & Build Plan

## ðŸ“ Where We Are Now

We have successfully architected and documented the **governance layer** for autonomous code evolution, but the **core execution engine** does not yet exist.

### âœ… What EXISTS (Governance & Specification)

| Component | Status | Location |
|-----------|--------|----------|
| **SDGC Documentation** | âœ… Complete | `setup.md` |
| **JSON Schema Definitions** | âœ… Complete | `schemas.json` |
| **AGI Manifesto** | âœ… Complete | `MANIFESTO.md` |
| **Multi-Model Consensus Spec** | âœ… Documented | `MANIFESTO.md` Â§ "Multi-Model Consensus" |
| **Validation Pipeline Spec** | âœ… Documented | `MANIFESTO.md` Â§ "Hallucination Validation" |
| **Architecture Diagrams** | âœ… Complete | `MANIFESTO.md` |

### âŒ What DOESN'T EXIST (The Actual System)

| Component | Status | Priority |
|-----------|--------|----------|
| **Context Loader** | âš ï¸ Not implemented | HIGH |
| **File Analysis Engine** | âš ï¸ Not implemented | HIGH |
| **LLM Integration Layer** | âš ï¸ Not implemented | CRITICAL |
| **Multi-Model Consensus Engine** | âš ï¸ Not implemented | CRITICAL |
| **Validation Sandbox** | âš ï¸ Not implemented | HIGH |
| **GitHub Integration** | âš ï¸ Not implemented | MEDIUM |
| **Firebase Memory Layer** | âš ï¸ Not implemented | MEDIUM |
| **Commit Engine** | âš ï¸ Not implemented | HIGH |
| **Main Event Loop** | âš ï¸ Not implemented | CRITICAL |

---

## ðŸŽ¯ The Irony

We have built:
- A schema for validating configurations that don't exist
- Documentation for tools (`sdgc-cli`) that aren't real
- A manifesto explaining a system that hasn't been coded
- Governance for a process that isn't running

**This is like building:**
- Building permits before the building
- Traffic laws before inventing cars
- A Constitution before forming a government

**Which is actually... smart?** We've specified the constraints before writing code, ensuring we can't build something ungovernable.

---

## ðŸš€ Build Roadmap

### Phase 1: MVP (Minimum Viable Sovereign)
**Goal:** Get ONE file to evolve successfully through the full pipeline

```
[ ] 1. Context Loader
    â””â”€> Read README.md
    â””â”€> Parse TODO.txt
    â””â”€> Cache in local JSON (skip Firebase for now)

[ ] 2. Single-Model Generator
    â””â”€> Send file + context to LLM (Llama 8b)
    â””â”€> Parse response (handle hallucination formats)
    â””â”€> Extract code mutation

[ ] 3. Basic Validator
    â””â”€> Syntax check (try to parse)
    â””â”€> Save to temp file
    â””â”€> Run linter if available

[ ] 4. File Writer
    â””â”€> Apply change to actual file
    â””â”€> Log what changed

[ ] 5. Manual Review Loop
    â””â”€> Show proposed change
    â””â”€> Ask human: approve/reject
    â””â”€> Learn from feedback
```

**Success Criteria:** One file gets improved, human approves it, change is applied.

---

### Phase 2: Consensus Layer (v94.1 Core)
**Goal:** Multi-model voting prevents bad hallucinations

```
[ ] 1. Multi-Model Integration
    â””â”€> Add 2nd model (Llama 70b or Claude)
    â””â”€> Generate 2-3 proposals per file
    
[ ] 2. Critic Agent
    â””â”€> Heavy model evaluates all proposals
    â””â”€> Scores on: security, performance, alignment, architecture
    â””â”€> Selects winner OR rejects all

[ ] 3. Consensus Threshold
    â””â”€> Only apply if confidence > 60%
    â””â”€> Log rejected proposals for analysis

[ ] 4. Conflict Resolution
    â””â”€> If models disagree significantly, flag for human review
```

**Success Criteria:** Bad hallucinations get filtered out, good ones pass through.

---

### Phase 3: Autonomous Operation
**Goal:** System runs without human intervention (but supervised)

```
[ ] 1. Automated Testing
    â””â”€> Run tests after each mutation
    â””â”€> Rollback if tests fail
    
[ ] 2. Performance Benchmarking
    â””â”€> Compare before/after execution time
    â””â”€> Reject changes that slow things down

[ ] 3. GitHub Integration
    â””â”€> Read files from repo
    â””â”€> Commit approved changes
    â””â”€> Update TODO list

[ ] 4. Firebase Memory
    â””â”€> Store all mutations (success + failure)
    â””â”€> Query past patterns before new mutations
    â””â”€> Enable continual learning
```

**Success Criteria:** System can run for 24 hours, make improvements, not break anything.

---

### Phase 4: Meta-Learning
**Goal:** System improves its own improvement process

```
[ ] 1. Pattern Recognition
    â””â”€> Analyze which types of mutations succeed
    â””â”€> Learn preferences from past decisions
    
[ ] 2. Strategy Adaptation
    â””â”€> Adjust temperature based on file type
    â””â”€> Use different models for different tasks

[ ] 3. Self-Modification
    â””â”€> Allow system to propose changes to its own code
    â””â”€> Heavy validation + human review required
```

**Success Criteria:** System gets better at evolution over time (measurable improvement in acceptance rate).

---

## ðŸ”§ Technical Stack (Proposed)

### Core Runtime
- **Language:** Python (for AI integration) or Node.js (for Firebase)
- **LLM Provider:** Cerebras / Groq / Together / OpenRouter
- **Models:** 
  - Fast: Llama 3.1 8B (generation)
  - Smart: Llama 3.3 70B (critique)
  - Optional: Claude/GPT-4 (tie-breaker)

### Infrastructure
- **Memory:** Firebase Firestore (as spec'd) or PostgreSQL
- **Code Host:** GitHub (via API)
- **Testing:** Docker containers (isolated execution)
- **Monitoring:** Simple logging â†’ stdout/file

### Development Environment
```bash
# Proposed structure
sovereign/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ context_loader.py      # Reads README/TODO
â”‚   â”œâ”€â”€ analyzer.py             # File analysis
â”‚   â”œâ”€â”€ generator.py            # LLM calls
â”‚   â”œâ”€â”€ consensus.py            # Multi-model voting (v94.1)
â”‚   â”œâ”€â”€ validator.py            # Syntax/tests/performance
â”‚   â”œâ”€â”€ committer.py            # Apply changes
â”‚   â””â”€â”€ memory.py               # Firebase/DB integration
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml             # System configuration
â”‚   â””â”€â”€ schemas.json            # (Already exists)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ MANIFESTO.md            # (Already exists)
â”‚   â”œâ”€â”€ setup.md                # (Already exists - SDGC)
â”‚   â””â”€â”€ ARCHITECTURE.md         # (To be created)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ ...                     # Unit tests for each component
â””â”€â”€ main.py                     # Event loop
```

---

## ðŸŽ“ Key Design Decisions

### 1. **Governance First, Code Second**
We intentionally built the rules before the system. This prevents "move fast and break things" mentality in AGI development.

### 2. **Human-in-the-Loop (Initially)**
Phase 1 requires human approval. Phase 3 removes this. This gradual autonomy is safer than "set it and forget it."

### 3. **Multi-Model Consensus is Non-Negotiable**
Single-model hallucinations are too risky. The consensus layer (v94.1 innovation) is what makes this safe.

### 4. **Embodied Learning**
The system acts in a real environment (the codebase), experiences consequences (tests pass/fail), and learns from outcomes. This is crucial for AGI.

### 5. **Incremental Self-Modification**
System can eventually modify itself, but only after proving it can safely modify other code first.

---

## ðŸ“Š Success Metrics

How do we know if Sovereign is working?

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Acceptance Rate** | >40% | % of proposals accepted by consensus/human |
| **Bug Introduction Rate** | <5% | % of mutations that break tests |
| **Code Quality Improvement** | +10% | Linter score, complexity metrics |
| **Feature Discovery** | >2/week | New capabilities emerged from mutations |
| **Self-Improvement** | Increasing | Acceptance rate trends upward over time |

---

## ðŸš§ Current Blockers

### To Start Building, We Need:

1. **LLM API Access**
   - [ ] Get API key (Cerebras / Groq / OpenRouter)
   - [ ] Test basic completion calls
   - [ ] Verify multi-model access

2. **Development Environment**
   - [ ] Set up Python/Node project
   - [ ] Install dependencies
   - [ ] Configure linters/formatters

3. **Test Repository**
   - [ ] Create sandbox repo
   - [ ] Add sample code files
   - [ ] Write initial README/TODO

4. **Validation Infrastructure**
   - [ ] Set up Docker (for sandboxing)
   - [ ] Create test suite
   - [ ] Define linter rules

---

## ðŸ¤” Open Questions

1. **What code should Sovereign evolve first?**
   - Its own codebase? (meta)
   - A simple utility library? (safe)
   - A real project? (ambitious)

2. **How aggressive should temperature be?**
   - Start at 0.8 (safer) or 1.5 (more creative)?
   - Adapt based on file type?

3. **What's the minimum viable consensus?**
   - 2 models? 3 models? 5 models?
   - What confidence threshold? (60%? 75%?)

4. **How do we measure "alignment"?**
   - Does the mutation match README goals?
   - Manual review? AI evaluation?

5. **When do we enable self-modification?**
   - After 100 successful mutations?
   - After 1000?
   - Never?

---

## ðŸŽ¬ Next Actions

**To begin implementation:**

```bash
# 1. Initialize project
mkdir sovereign-v94
cd sovereign-v94
python -m venv venv
source venv/bin/activate

# 2. Install dependencies
pip install anthropic openai groq firebase-admin pytest

# 3. Create minimal files
touch main.py context_loader.py generator.py
touch README.md TODO.txt config.yaml

# 4. Write Phase 1 code
# (See Phase 1 checklist above)

# 5. Test with one file
python main.py --file test.py --dry-run
```

---

## ðŸ’­ Final Thought

**We've built the Constitution before forming the Government.**

Now it's time to actually elect some representatives (spawn the processes), pass some laws (execute the mutations), and see if this democracy of models can create something greater than the sum of its parts.

The governance is ready.

**Time to build the thing being governed.** ðŸš€

---

_Status: Governance Complete, Implementation Pending_  
_Version: v94.1.0-alpha_  
_Last Updated: 2025 (Human-written, ironically)_  
_Next Step: Write `main.py`_







# $\Psi$ PROTOCOL (v94.1): GOVERNANCE STATE INTEGRITY MODEL (GSIM)

## ABSTRACT: Finality Anchor & Integrity Halt

The Governance State Integrity Model (GSIM) serves as the system's **Attestation Anchor** and zero-tolerance integrity layer, establishing and enforcing system state finality ($\Psi_{\text{final}}$). Integrity is defined by the **Foundational Axiom Set (GAX)**. Any detected violation results in immediate, non-recoverable system isolation via an **Integrity Halt (IH)**, managed exclusively by the Failure State Management Utility (FSMU).

---

## I. FOUNDATIONAL INTEGRITY CONTRACT: GAX & CRITICAL FAILURE TAXONOMY (P-SET)

The system must strictly adhere to the GAX constraints. All operational deviations map directly to the P-SET, mandating the IH protocol.

### I.I. GOVERNANCE AXIOM MATRIX (GAX)

| ID | Axiom | Enforcement Primitive | System Integrity Gate(s) | Violates P-SET Class | Primary Utility Anchor |
|:---:|:---|:---|:---:|:---:|:---:|
| **GAX I** | Determinism & Attestation | Cryptographic State Proof (Finality). | G2, G3 | R03, C04 | AASS, CMAC, CAPR |
| **GAX II** | Resource Boundedness | ACVM Constraint Compliance. | G1 | M02 | DRO, ISVA |
| **GAX III** | Immutability | G0 Configuration Hash-Lock (Seal). | G0, G1, G2 | M02 | EMSU, CDA, GAR |
| **GAX IV** | Temporal Sequence | GSEP-C Linear Order Enforcement. | G1, G2, G3 | M01 | PIM, RTOM |
| **GAX V** | Structural Integrity | Pre-flight Schema Validation. | PRE-G0 | M02 | CSV |

### I.II. CRITICAL FAILURE TAXONOMY (P-SET)

| Failure ID | Taxonomy | Mandate Failure | Immediate IH Trigger Description | Sub-State Action (IH Remediation) |
|:---:|:---|:---:|:---|:---:|
| **P-M01** | Temporal Sequence Fault | GAX IV | GSEP-C phase drift, timeline violation, or non-adherence to ordered execution logic. | PIM State Freeze |
| **P-M02** | Structural/Constraint Violation | GAX II, III, V | Resource boundary overrun (Time/Memory) or detected configuration drift from G0 Seal. | CDA Isolation / DRO Throttling |
| **P-R03** | Finality Compromise | GAX I | Failure to cryptographically prove system state ($\Psi_{\text{final}}$) repeatability and deterministic outcome. | AASS/EPRU Data Dump |
| **P-C04** | Compliance Drift Fault | GAX I | Attested behavior deviation from the sealed `CMAC` contract during runtime execution. | CMAC Termination / Hard IH |

---

## II. GSEP-C EXECUTION PIPELINE: STAGE & GATE ENFORCEMENT

The Governance State Execution Pipeline (GSEP-C) enforces linear progression via mandatory integrity gates, ensuring coupled GAX adherence at every phase.

| Gate | Stage Scope | Integrity Focus | Enforced Axioms | Primary Control Utility |
|:---:|:---|:---:|:---:|:---:|
| **PRE-G0** | S00 (Pre-flight) | Structural Schema Validation. | GAX V | CSV |
| **G0** | S00 (Sealing) | Configuration Immutability Lock (Hash Generation). | GAX III | EMSU, IKLM, GAR |
| **G1** | S01-S07 | Input State/Resource Boundary Checks & Validation (ISB Acceptance). | GAX II, IV | DRO, DHC, ISVA |
| **G2** | S08-S11 | Behavioral Compliance Attestation & Runtime Immutability Delta Check. | GAX I, IV, III | PIM, RTOM, CDA, CMAC |
| **G3** | S12-S14 | State Finalization & Deterministic Proof Generation. | GAX I, IV | AASS, PIM |

---

## III. SOVEREIGN UTILITIES MATRIX

All critical utilities categorized by Mission Cluster: State Enforcement, Temporal Oversight, and Crisis Management.

| Acronym | Utility Definition | Cluster | GAX Focus | Core Operational Function |
|:---:|:---|:---:|:---:|:---:|
| **AASS** | Audit & Signing Service | Crisis Management | GAX I | Cryptographically signs the Forensic Data Log Snapshot (FDLS) upon IH (Proof of Halt: P-R03 mitigation). |
| **CAPR** | Cryptographic Attestation Policy Registrar | State Enforcement | GAX I | Cryptographically anchors operational model hash to the G0 Seal for CMAC validation. |
| **CDA** | Configuration Delta Auditor | State Enforcement | GAX III | Monitors runtime state against the G0 Seal, detecting configuration drift (P-M02). |
| **CMAC** | Compliance Model Attestation Component | State Enforcement | GAX I, II | Runtime attestation of execution traces against sealed contracts (P-C04 enforcement). |
| **CSV** | Configuration Schema Validator | Temporal Oversight | GAX V | Ensures pre-execution file structures meet architectural requirements (P-M02 prevention). |
| **DHC** | Data Harvesting Component | Temporal Oversight | GAX II | Input State Buffer (ISB) acquisition and foundational data structure checks. |
| **DRO** | Dynamic Resource Orchestrator | State Enforcement | GAX II | ACVM Threshold Enforcement, preventing resource boundary overruns (P-M02 mitigation). |
| **EMSU** | Epoch Manifest Utility | Temporal Oversight | GAX III | Executes the G0 Seal generation (Configuration Hash-Lock). |
| **EPRU** | Execution Post-Mortem Utility | Crisis Management | GAX I | Securely archives the signed, immutable forensic halt data. |
| **FSMU** | Failure State Management Utility | Crisis Management | N/A | Primary executor of the Integrity Halt (IH) protocol. |
| **GAR** | Governance Artifacts Registrar | State Enforcement | GAX III | Central repository for all G0 configuration seals, tracking integrity status against the artifact manifest. |
| **IKLM** | Identity & Key Lifecycle Manager | State Enforcement | GAX I, III | Secure Key Management for state finality proofs and G0 configuration seals. |
| **ISVA** | Input State Validation Agent | Temporal Oversight | GAX II, IV | G1 granular input validation against defined policy (`config/isva_validation_policy.json`). |
| **PIM** | Protocol Integrity Manager | State Enforcement | GAX IV | GSEP-C Orchestration, sequencing control, and IH flow direction (P-M01 prevention). |
| **RTOM** | Real-Time Operational Monitor | Temporal Oversight | GAX IV | Low-latency metric acquisition and immediate failure state identification (P-M01 monitoring). |

---

## IV. IH PROTOCOL: FSMU MANDATE & ISOLATION STATE

The FSMU executes the non-reversible IH process to ensure immediate isolation and provable failure state:

1.  **Freeze & Capture:** PIM/FSMU generates Forensic Data & Log Snapshot (FDLS) per `protocol/telemetry_fdls_spec.yaml`.
2.  **Seal & Attest:** AASS receives FDLS and provides the mandatory cryptographic signing (Proof of Halt), satisfying GAX I.
3.  **Archive:** Signed FDLS is routed to EPRU for immutable storage.
4.  **Isolate & Purge:** FSMU triggers immediate resource purging and hardware isolation procedures, preventing subsequent state change propagation.

---

## V. GSIM ARTIFACT REGISTRY & FINALITY ANCHOR

All system artifacts essential for GAX enforcement are registered and tracked by the **Governance Artifacts Registrar (GAR)**. Integrity relies on maintaining the GAX III G0 Seal on these configuration files. The GAR is the primary utility responsible for verifying the integrity against the `artifact_manifest.json` root.

| Artifact Path | Verification Utility | GAX Responsibility | Seal Status | Rationale / Purpose |
|:---:|:---|:---:|:---:|:---:|
| `protocol/artifact_manifest.json` | GAR | GAX III | SEALED | Primary index containing paths and expected G0 hashes for all sealed configurations (Root of Trust). |
| `config/acvm_bounds.json` | DRO/GAR | GAX II | SEALED | Maximum resource consumption thresholds. |
| `config/gsep_orchestrator.json` | PIM/GAR | GAX IV | SEALED | Defines linear stage progression and execution transitions. |
| `config/cmac_compliance_spec.json` | CMAC/GAR | GAX I/II | SEALED | Defines mandatory behavioral constraints/metrics. |
| `config/isva_validation_policy.json` | ISVA/GAR | GAX II/IV | SEALED | Micro-validation constraints for Input State Buffer (ISB). |
| `protocol/attestation_policy_map.json` | CAPR/GAR | GAX I | SEALED | Maps sealed execution model hashes to their mandated IDs. |
| `protocol/cryptographic_manifest.json` | AASS/IKLM | GAX I | SEALED | Specifies required signing key dependencies for Determinism Proof. |
| `protocol/gax_master.yaml` | CSV/GAR | GAX V | SEALED | Core structural definition for the entire GAX matrix. |
| `config/key_rotation_schedule.json` | IKLM/GAR | GAX I/III | SEALED | Defines cryptographic key rotation and artifact hashing procedures. |
| `protocol/telemetry_fdls_spec.yaml` | FSMU | IH Execution | UNSEALED | Schema defining failure output logs (Schema for runtime output only). |
