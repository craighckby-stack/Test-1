import json
import logging
import datetime
from typing import Dict, Optional, List, TypedDict, Any

# --- Governance Data Schema Definitions ---
# Formalizing data structures for high-fidelity internal state management (ADA V94 Standard)

class ResourceConstraints(TypedDict):
    """Defines the standard set of acceptable resource limits for an artifact execution unit.
    Requires external configuration loading via ConfigurationGateway.
    """
    cpu_max_msec: int
    memory_peak_mb: int
    io_latency_max_ms: int
    max_dependency_depth: int # Added explicit governance metric

class CostVector(TypedDict):
    """Defines the measured computational cost vectors generated by simulation trace."""
    cpu_max_msec: float
    memory_peak_mb: float
    io_latency_avg_ms: float
    external_api_calls: int
    
class ArtifactFootprint(TypedDict):
    """Defines the standard internal representation of an audited artifact.
    Used internally by the dependency_ledger.
    """
    artifact_id: str
    cost_vectors: CostVector
    implicit_dependencies: List[str]
    audit_timestamp: str

# Initialize standardized governance logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

class ArtifactDependencyAuditor:
    """
    Audits computational footprints and dependencies of deployed artifacts against 
    SVP manifests and defined resource constraints, strictly utilizing TypedDict schemas.
    """
    DEFAULT_MANIFEST_PATH = "artifacts/L2_SVP.json"
    
    def __init__(self, manifest_source: str = DEFAULT_MANIFEST_PATH,
                 constraints: Optional[ResourceConstraints] = None):
        """Initializes ADA, formalizing constraints handling."""
        self.manifest_source = manifest_source
        # Strict type definition for internal ledger state
        self.dependency_ledger: Dict[str, ArtifactFootprint] = {}
        
        if constraints is None:
            # Placeholder defaults if ConfigurationGateway has not injected policies
            self.resource_constraints: ResourceConstraints = {
                "cpu_max_msec": 5000,
                "memory_peak_mb": 1024,
                "io_latency_max_ms": 50,
                "max_dependency_depth": 3
            }
        else:
            self.resource_constraints = constraints


    def _load_svp_payload(self) -> Optional[Dict]:
        """Loads the current L2 artifact manifest using robust error handling."""
        try:
            with open(self.manifest_source, 'r') as f:
                payload = json.load(f)
                logger.debug(f"SVP manifest loaded successfully from {self.manifest_source}")
                return payload
        except FileNotFoundError:
            # Critical error for audit start
            logger.error(f"SVP manifest not found at {self.manifest_source}. Aborting analysis initiation.")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON structure in SVP manifest: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error during SVP load: {e}")
            return None

    def map_computational_footprint(self, artifact_id: str, trace_data: Dict[str, Any]) -> Optional[ArtifactFootprint]:
        """
        Analyzes simulation trace data (L3 preparatory) and maps resource usage 
        according to the ArtifactFootprint schema.
        """
        if not artifact_id or not trace_data:
            logger.warning("Attempted to map footprint with empty artifact_id or trace_data.")
            return None

        # Type casting and zero-defaulting to strictly adhere to CostVector schema
        cost_vectors: CostVector = {
            "cpu_max_msec": float(trace_data.get("peak_cpu", 0)),
            "memory_peak_mb": float(trace_data.get("peak_mem", 0)),
            "io_latency_avg_ms": float(trace_data.get("avg_io", 0)),
            "external_api_calls": int(trace_data.get("external_calls", 0))
        }

        footprint: ArtifactFootprint = {
            "artifact_id": artifact_id,
            "cost_vectors": cost_vectors,
            "implicit_dependencies": self._extract_implicit_dependencies(trace_data.get("code_snapshot", {})),
            "audit_timestamp": self._get_timestamp()
        }
        
        self.dependency_ledger[artifact_id] = footprint
        logger.debug(f"Mapped footprint for artifact: {artifact_id}")
        return footprint

    def _extract_implicit_dependencies(self, snapshot: Dict) -> List[str]:
        """
        Identifies non-declared dependencies using simulated static analysis heuristics.
        Uses set comprehension and mapping for efficient identification.
        """
        dependency_map = {
            "module_reference_ATM": "AGI_TRUST_METRICS_SYSTEM",
            "utilizes_config": "CONFIGURATION_GATEWAY_V94",
            "uses_high_risk_syscall": "OS_KERNEL_INTEROP_INTERFACE" # Added security heuristic
        }

        implicit_set = {
            dependency_map[key]
            for key, value in snapshot.items()
            if key in dependency_map and value is True
        }
             
        return sorted(list(implicit_set))

    def generate_risk_input_vector(self, artifact_id: str) -> Dict[str, Any]:
        """
        Formats the dependency audit data for consumption by the Multi-Constraint Risk Assessor (MCRA S-02).
        Provides granular constraint violation details and weighted overhead scoring.
        """
        if artifact_id not in self.dependency_ledger:
            logger.error(f"Risk vector generation failed: Artifact ID {artifact_id} not audited.")
            return {"error": "Artifact ID not audited.", "status": "FAIL"}

        data = self.dependency_ledger[artifact_id]
        costs = data['cost_vectors']

        violation_status = self._check_budget_violation(costs)
        
        # Weighted normalization against defined constraints
        cpu_norm = costs['cpu_max_msec'] / self.resource_constraints.get('cpu_max_msec', 1)
        mem_norm = costs['memory_peak_mb'] / self.resource_constraints.get('memory_peak_mb', 1)
        
        # Weighted composite score (Prioritizing CPU/Memory pressure)
        overhead_score = ((cpu_norm * 0.6) + (mem_norm * 0.4)) * 100 

        risk_input = {
            "analysis_time_utc": data['audit_timestamp'],
            "cost_status": violation_status,
            "computational_overhead_score": round(overhead_score, 2),
            "dependency_breach_count": len(data['implicit_dependencies']),
            "critical_dependencies_list": data['implicit_dependencies'],
            "raw_cost_data": costs
        }
        
        return risk_input

    def _get_timestamp(self) -> str:
        """Returns high-fidelity ISO-standardized UTC timestamp for traceability."""
        return datetime.datetime.utcnow().isoformat() + 'Z'

    def _check_budget_violation(self, costs: CostVector) -> Dict[str, Any]:
        """
        Checks costs against constraints and returns a detailed status dictionary.
        """
        status = {
            "exceeded_any": False,
            "cpu_breach": costs["cpu_max_msec"] > self.resource_constraints["cpu_max_msec"],
            "memory_breach": costs["memory_peak_mb"] > self.resource_constraints["memory_peak_mb"],
            "io_breach": costs["io_latency_avg_ms"] > self.resource_constraints.get("io_latency_max_ms", float('inf'))
        }
        
        if status["cpu_breach"] or status["memory_breach"] or status["io_breach"]:
             status["exceeded_any"] = True
             logger.warning(f"Budget violation detected: CPU={status['cpu_breach']}, Memory={status['memory_breach']}")
             
        return status